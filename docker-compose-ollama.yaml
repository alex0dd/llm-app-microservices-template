networks:
  shared_net:
  backend_llm_net: # network for backend and llm_server communication

services:
  nginx:
    image: nginx:stable-alpine
    ports:
      - "80:80"
    networks:
      - shared_net
    volumes:
      - "./nginx.conf:/etc/nginx/nginx.conf"
    depends_on:
      - backend
      - frontend
      - llm_server
  frontend:
    build:
      context: frontend
    ports:
      - "3000:3000"
    networks:
      - shared_net
    depends_on:
      - llm_server
      - backend
  backend:
    build:
      context: backend
    ports:
      - "5192:5192"
    networks:
      - shared_net
      - backend_llm_net  # Network for backend-llm communication
    depends_on:
      - llm_server
    environment:
      - LLM_SERVER_TYPE=ollama
  llm_server:
    user: root
    image: ollama/ollama:latest
    entrypoint: /bin/sh /ollama/run_ollama.sh
    ports:
      - "11434:11434"
    volumes:
      - "./ollama:/ollama"
      - "~/.ollama/models:/root/.ollama/models"
    networks:
      - backend_llm_net  # Only accessible by backend